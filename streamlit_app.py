import streamlit as st

import time

import os

import google.generativeai as genai

import fitzÂ  # PyMuPDF

from PIL import Image

import json

import re

from concurrent.futures import ThreadPoolExecutor, as_completed

import hashlib

import requests

import tempfile



# --- GOOGLE OAUTH LIBRARIES ---

try:

Â  Â  from google_auth_oauthlib.flow import Flow

Â  Â  from googleapiclient.discovery import build

except ImportError:

Â  Â  st.error("""

Â  Â  Â  Â  **Required Google libraries not found!**

Â  Â  Â  Â  Please install the necessary packages using pip:

Â  Â  Â  Â  ```bash

Â  Â  Â  Â  pip install google-auth-oauthlib google-api-python-client

Â  Â  Â  Â  ```

Â  Â  Â  Â  The app cannot continue without these dependencies. Please install and refresh.

Â  Â  """)

Â  Â  st.stop()



# --- CONFIGURATION & CONSTANTS ---

MAX_FILES = 20

MAX_TOTAL_SIZE_MB = 150

MAX_AUDIO_SIZE_MB = 1024 # Gemini has a larger limit, but good to have a cap.

CHUNK_SIZE = 500Â  # words

CHUNK_OVERLAP = 50 # words



# --- PAGE CONFIGURATION ---

st.set_page_config(

Â  Â  page_title="Vekkam Engine",

Â  Â  page_icon="ðŸ§ ",

Â  Â  layout="wide",

Â  Â  initial_sidebar_state="expanded"

)



# --- API SELF-DIAGNOSIS & UTILITIES ---

def check_gemini_api():

Â  Â  try:

Â  Â  Â  Â  genai.get_model('models/gemini-2.5-flash-lite')

Â  Â  Â  Â  return "Valid"

Â  Â  except Exception as e:

Â  Â  Â  Â  st.sidebar.error(f"Gemini API Key in secrets is invalid: {e}")

Â  Â  Â  Â  return "Invalid"



def resilient_json_parser(json_string):

Â  Â  try:

Â  Â  Â  Â  match = re.search(r'\{.*\}', json_string, re.DOTALL)

Â  Â  Â  Â  if match: return json.loads(match.group(0))

Â  Â  Â  Â  return None

Â  Â  except json.JSONDecodeError:

Â  Â  Â  Â  st.error("Fatal Error: Could not parse a critical AI JSON response."); return None



def chunk_text(text, source_id, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):

Â  Â  if not text: return []

Â  Â  words = text.split()

Â  Â  chunks = []

Â  Â  for i in range(0, len(words), chunk_size - overlap):

Â  Â  Â  Â  chunk_words = words[i:i + chunk_size]

Â  Â  Â  Â  chunk_text = " ".join(chunk_words)

Â  Â  Â  Â  chunk_hash = hashlib.md5(chunk_text.encode()).hexdigest()[:8]

Â  Â  Â  Â  chunk_id = f"{source_id}::chunk_{i//(chunk_size-overlap)}_{chunk_hash}"

Â  Â  Â  Â  chunks.append({"chunk_id": chunk_id, "text": chunk_text})

Â  Â  return chunks



# --- CONTENT PROCESSING & AGENTIC WORKFLOW ---

def process_source(file, source_type):

Â  Â  try:

Â  Â  Â  Â  source_id = f"{source_type}:{file.name}"

Â  Â  Â  Â  model = genai.GenerativeModel('models/gemini-2.5-flash-lite')



Â  Â  Â  Â  if source_type == 'transcript':

Â  Â  Â  Â  Â  Â  with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.name)[1]) as tmp:

Â  Â  Â  Â  Â  Â  Â  Â  tmp.write(file.getvalue())

Â  Â  Â  Â  Â  Â  Â  Â  tmp_path = tmp.name

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  audio_file = genai.upload_file(path=tmp_path)

Â  Â  Â  Â  Â  Â  Â  Â  while audio_file.state.name == "PROCESSING":

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  time.sleep(2)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  audio_file = genai.get_file(audio_file.name)

Â  Â  Â  Â  Â  Â  Â  Â  if audio_file.state.name == "FAILED":

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return {"status": "error", "source_id": source_id, "reason": "Gemini file processing failed."}

Â  Â  Â  Â  Â  Â  Â  Â  response = model.generate_content(["Transcribe this audio file.", audio_file])

Â  Â  Â  Â  Â  Â  Â  Â  chunks = chunk_text(response.text, source_id)

Â  Â  Â  Â  Â  Â  Â  Â  return {"status": "success", "source_id": source_id, "chunks": chunks}

Â  Â  Â  Â  Â  Â  finally:

Â  Â  Â  Â  Â  Â  Â  Â  os.unlink(tmp_path)

Â  Â  Â  Â  elif source_type == 'image':

Â  Â  Â  Â  Â  Â  image = Image.open(file)

Â  Â  Â  Â  Â  Â  response = model.generate_content(["Analyze this image...", image])

Â  Â  Â  Â  Â  Â  return {"status": "success", "source_id": source_id, "chunks": [{"chunk_id": f"{source_id}::chunk_0", "text": response.text}]}

Â  Â  Â  Â  elif source_type == 'pdf':

Â  Â  Â  Â  Â  Â  pdf_bytes = file.read()

Â  Â  Â  Â  Â  Â  doc = fitz.open(stream=pdf_bytes, filetype="pdf")

Â  Â  Â  Â  Â  Â  text = "".join(page.get_text() for page in doc)

Â  Â  Â  Â  Â  Â  chunks = chunk_text(text, source_id)

Â  Â  Â  Â  Â  Â  return {"status": "success", "source_id": source_id, "chunks": chunks}

Â  Â  except Exception as e:

Â  Â  Â  Â  return {"status": "error", "source_id": f"{source_type}:{file.name}", "reason": str(e)}



def generate_content_outline(all_chunks, existing_outline=None):

Â  Â  try:

Â  Â  Â  Â  model = genai.GenerativeModel('models/gemini-2.5-flash-lite')

Â  Â  Â  Â  prompt_chunks = [{"chunk_id": c['chunk_id'], "text_snippet": c['text'][:200] + "..."} for c in all_chunks]

Â  Â  Â  Â  instruction = "Analyze the content chunks and create a structured outline."

Â  Â  Â  Â  if existing_outline:

Â  Â  Â  Â  Â  Â  instruction = f"Analyze the NEW content chunks and suggest topics to ADD to the existing outline."

Â  Â  Â  Â  prompt = f"""

Â  Â  Â  Â  You are a curriculum designer. {instruction}

Â  Â  Â  Â  For each topic, you MUST list the `chunk_id`s that are most relevant.

Â  Â  Â  Â  Output ONLY a JSON object with a root key "outline", a list of objects. Each object must have keys "topic" (string) and "relevant_chunks" (list of strings).

Â  Â  Â  Â  **Existing Outline (for context):**

Â  Â  Â  Â  {json.dumps(existing_outline, indent=2) if existing_outline else "None"}

Â  Â  Â  Â  **Content Chunks:**

Â  Â  Â  Â  ---

Â  Â  Â  Â  {json.dumps(prompt_chunks, indent=2)}

Â  Â  Â  Â  """

Â  Â  Â  Â  response = model.generate_content(prompt)

Â  Â  Â  Â  return resilient_json_parser(response.text)

Â  Â  except Exception as e:

Â  Â  Â  Â  st.error(f"Outline Generation Error: {e}"); return None



def synthesize_note_block(topic, relevant_chunks_text, instructions):

Â  Â  try:

Â  Â  Â  Â  model = genai.GenerativeModel('models/gemini-2.5-flash-lite')

Â  Â  Â  Â  prompt = f"""

Â  Â  Â  Â  Write the notes for a single topic: "{topic}".

Â  Â  Â  Â  Use ONLY the provided source text. Adhere to the user's instructions. Format in Markdown.

Â  Â  Â  Â  **Instructions:** {instructions if instructions else "None"}

Â  Â  Â  Â  **Source Text:** {relevant_chunks_text}

Â  Â  Â  Â  """

Â  Â  Â  Â  response = model.generate_content(prompt)

Â  Â  Â  Â  return response.text

Â  Â  except Exception as e:

Â  Â  Â  Â  return f"Error synthesizing this block: {e}"



# --- AUTHENTICATION SETUP ---

def get_google_flow():

Â  Â  try:

Â  Â  Â  Â  client_config = {

Â  Â  Â  Â  Â  Â  "web": {

Â  Â  Â  Â  Â  Â  Â  Â  "client_id": st.secrets["google"]["client_id"],

Â  Â  Â  Â  Â  Â  Â  Â  "client_secret": st.secrets["google"]["client_secret"],

Â  Â  Â  Â  Â  Â  Â  Â  "auth_uri": "https://accounts.google.com/o/oauth2/auth",

Â  Â  Â  Â  Â  Â  Â  Â  "token_uri": "https://oauth2.googleapis.com/token",

Â  Â  Â  Â  Â  Â  Â  Â  "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",

Â  Â  Â  Â  Â  Â  Â  Â  "redirect_uris": [st.secrets["google"]["redirect_uri"]],

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  }

Â  Â  Â  Â  scopes = ["https://www.googleapis.com/auth/userinfo.profile", "https://www.googleapis.com/auth/userinfo.email", "openid"]

Â  Â  Â  Â  return Flow.from_client_config(client_config, scopes=scopes, redirect_uri=st.secrets["google"]["redirect_uri"])

Â  Â  except (KeyError, FileNotFoundError):

Â  Â  Â  Â  st.error("OAuth credentials are not configured correctly in st.secrets.")

Â  Â  Â  Â  st.stop()



# --- SESSION STATE & RESET ---

def reset_session():

Â  Â  for key in list(st.session_state.keys()):

Â  Â  Â  Â  if key not in ['user_info']:

Â  Â  Â  Â  Â  Â  del st.session_state[key]

Â  Â  st.session_state.current_state = 'upload'

Â  Â  st.session_state.all_chunks = []

Â  Â  st.session_state.extraction_failures = []

Â  Â  st.session_state.outline_data = []

Â  Â  st.session_state.final_notes = []



# --- MAIN APP LOGIC ---

def main():

Â  Â  st.sidebar.title("Vekkam Engine")



Â  Â  # Initialize session state & configure Gemini

Â  Â  if 'user_info' not in st.session_state:

Â  Â  Â  Â  st.session_state.user_info = None

Â  Â  try:

Â  Â  Â  Â  genai.configure(api_key=st.secrets["gemini"]["api_key"])

Â  Â  except (KeyError, FileNotFoundError):

Â  Â  Â  Â  st.error("Gemini API key is not configured in st.secrets.")

Â  Â  Â  Â  st.stop()



Â  Â  flow = get_google_flow()

Â  Â  query_params = st.query_params

Â  Â  auth_code = query_params.get("code")



Â  Â  if auth_code and not st.session_state.user_info:

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  flow.fetch_token(code=auth_code)

Â  Â  Â  Â  Â  Â  credentials = flow.credentials

Â  Â  Â  Â  Â  Â  user_info_service = build('oauth2', 'v2', credentials=credentials)

Â  Â  Â  Â  Â  Â  user_info = user_info_service.userinfo().get().execute()

Â  Â  Â  Â  Â  Â  st.session_state.user_info = user_info

Â  Â  Â  Â  Â  Â  st.query_params.clear()

Â  Â  Â  Â  Â  Â  st.rerun()

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  st.error(f"Failed to fetch token or user info: {e}")

Â  Â  Â  Â  Â  Â  st.session_state.user_info = None

Â  Â  Â  Â  Â  Â Â 

Â  Â  # --- Authentication Gate ---

Â  Â  if not st.session_state.user_info:

Â  Â  Â  Â  st.title("Welcome to Vekkam")

Â  Â  Â  Â  st.write("Sign in with Google to start synthesizing knowledge.")

Â  Â  Â  Â  auth_url, _ = flow.authorization_url(prompt='consent')

Â  Â  Â  Â  st.link_button("Sign in with Google", auth_url)

Â  Â  Â  Â  return



Â  Â  # --- Post-Login App ---

Â  Â  user = st.session_state.user_info

Â  Â  st.sidebar.image(user['picture'], width=80)

Â  Â  st.sidebar.subheader(f"Welcome, {user['given_name']}")

Â  Â  if st.sidebar.button("Logout"):

Â  Â  Â  Â  st.session_state.clear()

Â  Â  Â  Â  st.rerun()



Â  Â  st.sidebar.divider()

Â  Â  st.sidebar.subheader("API Status")

Â  Â  st.sidebar.write(f"Gemini 1.5 Pro: **{check_gemini_api()}**")



Â  Â  if 'current_state' not in st.session_state:

Â  Â  Â  Â  reset_session()

Â  Â Â 

Â  Â  # --- CORE APP VIEWS ROUTER ---

Â  Â  if st.session_state.current_state == 'upload':

Â  Â  Â  Â  show_upload_state()

Â  Â  elif st.session_state.current_state == 'workspace':

Â  Â  Â  Â  show_workspace_state()

Â  Â  elif st.session_state.current_state == 'synthesizing':

Â  Â  Â  Â  show_synthesizing_state()

Â  Â  elif st.session_state.current_state == 'results':

Â  Â  Â  Â  show_results_state()



def show_upload_state():

Â  Â  st.header("Upload Your Sources")

Â  Â  uploaded_files = st.file_uploader("Select files", accept_multiple_files=True, type=['mp3', 'm4a', 'wav', 'png', 'jpg', 'pdf'])

Â  Â  if st.button("Process Files", type="primary") and uploaded_files:

Â  Â  Â  Â  with st.spinner("Processing initial files... This can take a moment."):

Â  Â  Â  Â  Â  Â  process_files_and_chunks(uploaded_files)

Â  Â  Â  Â  st.session_state.current_state = 'workspace'

Â  Â  Â  Â  st.rerun()



def process_files_and_chunks(files_to_process):

Â  Â  results = []

Â  Â  with ThreadPoolExecutor() as executor:

Â  Â  Â  Â  futures = {executor.submit(process_source, f, 'transcript' if f.type.startswith('audio/') else 'image' if f.type.startswith('image/') else 'pdf'): f for f in files_to_process}

Â  Â  Â  Â  for future in as_completed(futures):

Â  Â  Â  Â  Â  Â  results.append(future.result())

Â  Â Â 

Â  Â  new_chunks = []

Â  Â  for r in [res for res in results if res and res['status'] == 'success']:

Â  Â  Â  Â  new_chunks.extend(r['chunks'])

Â  Â  st.session_state.all_chunks.extend(new_chunks)

Â  Â  st.session_state.extraction_failures.extend([r for r in results if r and r['status'] == 'error'])

Â  Â  return new_chunks



def show_workspace_state():

Â  Â  st.header("Vekkam Workspace")

Â  Â  col1, col2 = st.columns([2, 1])



Â  Â  with col1:

Â  Â  Â  Â  st.subheader("Controls & Outline")

Â  Â  Â  Â  if st.button("Generate / Regenerate Full Outline"):

Â  Â  Â  Â  Â  Â  with st.spinner("AI is analyzing all content..."):

Â  Â  Â  Â  Â  Â  Â  Â  outline_json = generate_content_outline(st.session_state.all_chunks)

Â  Â  Â  Â  Â  Â  Â  Â  if outline_json and "outline" in outline_json:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.outline_data = outline_json["outline"]

Â  Â  Â  Â  Â  Â  Â  Â  else: st.error("Failed to generate outline.")

Â  Â  Â  Â Â 

Â  Â  Â  Â  if 'outline_data' in st.session_state and st.session_state.outline_data:

Â  Â  Â  Â  Â  Â  initial_outline_text = "\n".join([item.get('topic', '') for item in st.session_state.outline_data])

Â  Â  Â  Â  Â  Â  st.session_state.editable_outline = st.text_area("Editable Outline:", value=initial_outline_text, height=300)

Â  Â  Â  Â  Â  Â  st.session_state.synthesis_instructions = st.text_area("Synthesis Instructions (Optional):", height=100)

Â  Â  Â  Â  Â  Â  if st.button("Synthesize Notes", type="primary"):

Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.current_state = 'synthesizing'

Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()

Â  Â Â 

Â  Â  with col2:

Â  Â  Â  Â  st.subheader("Source Explorer")

Â  Â  Â  Â  with st.expander("Add More Files"):

Â  Â  Â  Â  Â  Â  new_files = st.file_uploader("Upload more files", accept_multiple_files=True, key=f"uploader_{int(time.time())}")

Â  Â  Â  Â  Â  Â  if new_files:

Â  Â  Â  Â  Â  Â  Â  Â  new_chunks = process_files_and_chunks(new_files)

Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("AI is suggesting new topics..."):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  update_json = generate_content_outline(new_chunks, existing_outline=st.session_state.get('outline_data', []))

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if update_json and "outline" in update_json:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.outline_data.extend(update_json["outline"])

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success(f"Added {len(update_json['outline'])} new topic(s)!")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()



Â  Â  Â  Â  if st.session_state.get('all_chunks'):

Â  Â  Â  Â  Â  Â  with st.expander("Explore All Content Chunks", expanded=False):

Â  Â  Â  Â  Â  Â  Â  Â  for i, chunk in enumerate(st.session_state.all_chunks):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**Chunk ID:** `{chunk['chunk_id']}`")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text_area("", chunk['text'], height=100, key=f"chunk_viewer_{i}")



def show_synthesizing_state():

Â  Â  st.header("Synthesizing Note Blocks...")

Â  Â  st.session_state.final_notes = []

Â  Â  outline_topics = [line.strip() for line in st.session_state.editable_outline.split('\n') if line.strip()]

Â  Â  all_chunks_map = {chunk['chunk_id']: chunk['text'] for chunk in st.session_state.all_chunks}

Â  Â  original_outline_map = {item['topic']: item.get('relevant_chunks', []) for item in st.session_state.outline_data}



Â  Â  progress_bar = st.progress(0, "Starting synthesis...")

Â  Â  for i, topic in enumerate(outline_topics):

Â  Â  Â  Â  progress_bar.progress((i + 1) / len(outline_topics), f"Synthesizing: {topic}")

Â  Â  Â  Â  relevant_chunk_ids = original_outline_map.get(topic, [])

Â  Â  Â  Â  relevant_chunks_text = "\n\n---\n\n".join([all_chunks_map.get(cid, "") for cid in relevant_chunk_ids])

Â  Â  Â  Â  content = synthesize_note_block(topic, relevant_chunks_text, st.session_state.synthesis_instructions)

Â  Â  Â  Â  st.session_state.final_notes.append({"topic": topic, "content": content, "source_chunks": relevant_chunk_ids})

Â  Â Â 

Â  Â  st.session_state.current_state = 'results'

Â  Â  st.rerun()



def show_results_state():

Â  Â  st.header("Your Unified Notes")

Â  Â  if st.button("Back to Workspace"):

Â  Â  Â  Â  st.session_state.current_state = 'workspace'

Â  Â  Â  Â  st.rerun()

Â  Â  if st.button("Start New Session"):

Â  Â  Â  Â  reset_session()

Â  Â  Â  Â  st.rerun()



Â  Â  for i, note_block in enumerate(st.session_state.final_notes):

Â  Â  Â  Â  st.subheader(note_block['topic'])

Â  Â  Â  Â  st.markdown(note_block['content'])

Â  Â  Â  Â  if st.button("Regenerate this block", key=f"regen_{i}"):

Â  Â  Â  Â  Â  Â  with st.spinner("Regenerating block..."):

Â  Â  Â  Â  Â  Â  Â  Â  all_chunks_map = {chunk['chunk_id']: chunk['text'] for chunk in st.session_state.all_chunks}

Â  Â  Â  Â  Â  Â  Â  Â  relevant_chunks_text = "\n\n---\n\n".join([all_chunks_map.get(cid, "") for cid in note_block['source_chunks']])

Â  Â  Â  Â  Â  Â  Â  Â  new_content = synthesize_note_block(note_block['topic'], relevant_chunks_text, st.session_state.synthesis_instructions)

Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.final_notes[i]['content'] = new_content

Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()

Â  Â  Â  Â  with st.expander("View Source Chunks for this Block"):

Â  Â  Â  Â  Â  Â  st.json(note_block['source_chunks'])



if __name__ == "__main__":

Â  Â  main()
